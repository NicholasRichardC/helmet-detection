import cv2
import numpy as np
import xml.etree.ElementTree as ET
from skimage.feature import hog, local_binary_pattern
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
import glob

TRAIN_DIR = './Dataset2/train'
VAL_DIR = './Dataset2/valid'
MODEL_DIR = 'models/'

os.makedirs(MODEL_DIR, exist_ok=True)

FACE_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
PROFILE_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_profileface.xml'

# Helmet region parameters
HELMET_X_OFFSET = -0.45
HELMET_Y_OFFSET = -0.85
HELMET_WIDTH_SCALE = 1.7
HELMET_HEIGHT_SCALE = 1.5

FEATURE_SIZE = (64, 64)  # Resize helmet ROI
CLASS_MAP = {'helmet': 1, 'no_helmet': 0}

face_cascade = cv2.CascadeClassifier(FACE_CASCADE_PATH)
profile_cascade = cv2.CascadeClassifier(PROFILE_CASCADE_PATH)

def detect_faces(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))
    if len(faces) == 0:
        faces = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))
    return faces

def get_helmet_region(face_bbox, img_shape):
    fx, fy, fw, fh = face_bbox
    img_h, img_w = img_shape[:2]
    hx = max(0, int(fx + fw * HELMET_X_OFFSET))
    hy = max(0, int(fy + fh * HELMET_Y_OFFSET))
    hw = min(int(fw * HELMET_WIDTH_SCALE), img_w - hx)
    hh = min(int(fh * HELMET_HEIGHT_SCALE), img_h - hy)
    return (hx, hy, hw, hh)

# =======================
# FEATURE EXTRACTION
# =======================
def extract_helmet_features(helmet_roi):
    roi = cv2.resize(helmet_roi, FEATURE_SIZE)
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    gray = cv2.equalizeHist(gray)
    
    features = []
  
    hog_feat = hog(gray, orientations=9, pixels_per_cell=(8,8),
                   cells_per_block=(2,2), feature_vector=True)
    features.extend(hog_feat)
    
    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    for c in range(3):
        hist = cv2.calcHist([hsv], [c], None, [16], [0,256])
        hist = hist.flatten() / (hist.sum() + 1e-7)
        features.extend(hist)
    
    lbp = local_binary_pattern(gray, 8, 1, method='uniform')
    lbp_hist, _ = np.histogram(lbp.ravel(), bins=26, range=(0,26), density=True)
    features.extend(lbp_hist)
    
    edges = cv2.Canny(gray, 50, 150)
    edge_density = np.sum(edges > 0) / edges.size
    features.append(edge_density)
    
    features.append(np.mean(hsv[:,:,2])/255.0)
    features.append(np.std(hsv[:,:,1])/255.0)
    
    return np.array(features)

def load_xml_annotations(xml_file):
    annotations = []
    if not os.path.exists(xml_file):
        return annotations
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        for obj in root.findall('object'):
            name = obj.find('name').text.lower()
            bbox = obj.find('bndbox')
            xmin = int(bbox.find('xmin').text)
            ymin = int(bbox.find('ymin').text)
            xmax = int(bbox.find('xmax').text)
            ymax = int(bbox.find('ymax').text)
            annotations.append({'class': name, 'bbox': (xmin, ymin, xmax, ymax)})
    except Exception as e:
        print(f"Error parsing {xml_file}: {e}")
    return annotations

def create_dataset_improved(data_dir):
    X, y = [], []
    image_files = glob.glob(os.path.join(data_dir, '*.jpg'))
    print(f"Processing {len(image_files)} images from {data_dir}")
    
    for img_path in image_files:
        img = cv2.imread(img_path)
        if img is None:
            continue
        
        xml_path = img_path.replace('.jpg','.xml')
        annotations = load_xml_annotations(xml_path)
        if len(annotations) == 0:
            continue
        
        for ann in annotations:
            x1, y1, x2, y2 = ann['bbox']
            roi = img[y1:y2, x1:x2]
            if roi.shape[0] < 20 or roi.shape[1] < 20:
                continue
            try:
                features = extract_helmet_features(roi)
                X.append(features)
                y.append(CLASS_MAP[ann['class']])
            except:
                continue
    
    return np.array(X), np.array(y)

print("Creating training dataset...")
X_train, y_train = create_dataset_improved(TRAIN_DIR)

print("Creating validation dataset...")
X_val, y_val = create_dataset_improved(VAL_DIR)

if len(X_train) == 0:
    raise ValueError("No training data! Check XML files.")

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val) if len(X_val)>0 else None

# Train classifier
clf = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)
clf.fit(X_train_scaled, y_train)

# Evaluate
train_pred = clf.predict(X_train_scaled)
print(f"Training Accuracy: {accuracy_score(y_train, train_pred):.3f}")

if len(X_val_scaled)>0 and len(np.unique(y_val))>1:
    val_pred = clf.predict(X_val_scaled)
    print(f"Validation Accuracy: {accuracy_score(y_val, val_pred):.3f}")
    print(classification_report(y_val, val_pred, target_names=['no_helmet','helmet']))
    
    # Confusion matrix
    cm = confusion_matrix(y_val, val_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['no_helmet','helmet'],
                yticklabels=['no_helmet','helmet'])
    plt.title('Validation Confusion Matrix')
    plt.show()

joblib.dump(clf, os.path.join(MODEL_DIR, 'helmet_classifier.pkl'))
joblib.dump(scaler, os.path.join(MODEL_DIR, 'scaler.pkl'))

print("Training complete!")
